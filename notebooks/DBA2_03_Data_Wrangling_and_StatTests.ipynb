{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mschuessler/dba_lecture/blob/main/notebooks/DBA2_03_Data_Wrangling_and_StatTests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkiQ-ZiMjc7O"
      },
      "source": [
        "# Data Wrangling\n",
        "\n",
        "The Titanic dataset only had a few issues we needed to fix. To show you some typical issues that may need addressing, we will work with an additional dataset in this notebook: the answers you provided to the entry questionnaire for this class.\n",
        "\n",
        "**Note:** This time, we are importing pandas with an alias. So, instead of calling pandas directly, we will use the alias `pd`. This is not required but makes the code shorter and easier to write."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WewTTpfjc7P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use different data this time. This data was gathered using the survey tool Qualtrics. We can download this data at a url, which we are going to save in a variable to we do not have to type it repeately."
      ],
      "metadata": {
        "id": "NcA5J9d7ob7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_url = \"https://raw.githubusercontent.com/mschuessler/dba_lecture/refs/heads/main/data/dba_q2.csv\""
      ],
      "metadata": {
        "id": "CtAlvlD_osZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuUA4gS_jc7Q"
      },
      "source": [
        "# Reading Messy CSV Files\n",
        "I am calling the dataset `answ`, short for answers. [Unfortunately, Qualtrics does not export well-formatted CSV files](https://community.qualtrics.com/survey-platform-before-march-2021-56/export-to-csv-without-extra-header-rows-9491). Have a look at [dba_q2.csv](https://raw.githubusercontent.com/mschuessler/dba_lecture/refs/heads/main/data/dba_q2.csv) yourself:  \n",
        "The file contains three column headers instead of one:\n",
        "\n",
        "- **Line 1:** Contains the question label, which is a very suitable column name.  \n",
        "- **Lines 2–8:** Contain either metadata labels or the question that participants answered in plain text. This extends over several lines because one of the questions included spans multiple rows.  \n",
        "- **Line 9:** Contains the internal metadata for each question. We are not at all interested in this.\n",
        "\n",
        "Reading this file normally will result in either errors or strange data. We can see that we get a row (Row 0) that contains the names of the columns again, followed by a row that contains the metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqjK7I7ojc7Q"
      },
      "outputs": [],
      "source": [
        "pd.read_csv(q_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwCOggTqjc7Q"
      },
      "source": [
        "Now, one thing you might be thinking: \"Let's just delete these rows and we are good to go.\"  \n",
        "Let's try that and see what the issue with this approach might be.\n",
        "\n",
        "I am using the `loc` attribute here, which allows you to select rows by their row number. Again, we are starting to count at 0. The code reads like this:  \n",
        "Read \"dba_q2.csv\" and then select rows starting from 3 (zero-based: `2`) up to the end (`:`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99Wah6wQjc7Q"
      },
      "outputs": [],
      "source": [
        "answ.loc[2:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8ucm34ujc7Q"
      },
      "source": [
        "While the table looks fine now and we got rid of the unnecessary data, the problem we face is that pandas tries to guess the correct data type for each column upon reading the file.  \n",
        "\n",
        "Because the file initially contained strange character sequences, pandas likely did not detect any numbers. Instead, it assigned every column to the string data type (`dtype: object`). This would mean we would need to manually convert every column containing numbers—a tedious process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7D7N61fjc7Q"
      },
      "outputs": [],
      "source": [
        "pd.read_csv(q_url).loc[2:].dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH4uSGsWjc7R"
      },
      "source": [
        "The better way to handle this is to ignore the lines containing unwanted information as we read the file, _before_ pandas attempts to guess the data types.  \n",
        "\n",
        "This typically involves some trial and error. You adjust the number of rows to skip until the resulting dataframe looks as expected. For example, we know that 13 people answered the questionnaire, so we can adjust the number until we see exactly 13 responses in the table—no more, no less, and without any strange entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOc3e3i_jc7R"
      },
      "outputs": [],
      "source": [
        "answ = pd.read_csv(q_url, skiprows=[1, 2])\n",
        "answ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1PDYvBUjc7R"
      },
      "source": [
        "We can check the data types again, and now we see that there are indeed columns that pandas recognized as numeric, as they contain only numbers. This is much better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9O0WGU2Cjc7R"
      },
      "outputs": [],
      "source": [
        "answ.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ry4BVLMjc7R"
      },
      "source": [
        "Before we move on, I want to make you aware of some additional options that you may find useful when reading CSV files. I also strongly recommend [checking the pandas documentation for this function](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).  \n",
        "\n",
        "Additionally, if you explain the issues you are having with your data and include a snippet of the CSV file, ChatGPT usually makes good suggestions on how you can fix them. However, always double-check the results and use your common sense to verify that the data looks as expected.\n",
        "\n",
        "Here are some useful parameters for `read_csv`:\n",
        "\n",
        "- **`encoding`**: Default is `UTF-8`. Specify another character set (e.g., `latin1`, `iso-8859-15`, `cp1252`).\n",
        "- **`sep`**: Default is `\",\"`. Define a different column delimiter (e.g., `\";\"`).\n",
        "- **`decimal`**: Default is `\".\"`. Specify another decimal indicator (e.g., `\",\"`).\n",
        "- **`quotechar`**: Default is `'\"'`. Define a different character for quoted fields (e.g., `\"'\"`).\n",
        "- **`names`**: Default is `None`. Provide a list of column names for files without headers.\n",
        "- **`skip_blank_lines`**: Default is `True`. Skip empty lines automatically.\n",
        "- **`skiprows`**: Default is `None`. Specify a list of line numbers to ignore (only non-empty lines are counted if `skip_blank_lines` is `True`).\n",
        "- **`usecols`**: Default is `None`. Define the columns to be read; all others will be ignored.\n",
        "- **`parse_dates`**: Default is `False`. List columns to be read as dates. Use `date_format` to specify how dates are parsed.\n",
        "- **`dtype`**: Default is inferred. Define data types for each column.\n",
        "\n",
        "In this example, we only needed to set the `skiprows` option because all the other defaults were suitable for our file. We can demonstrate this by explicitly setting these options, which results in the same outcome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7llCU2rjc7R"
      },
      "outputs": [],
      "source": [
        "answ = pd.read_csv(\n",
        "    q_url,\n",
        "    encoding='utf-8',         # Default encoding\n",
        "    sep=',',                  # Default separator\n",
        "    decimal='.',              # Default decimal indicator\n",
        "    quotechar='\"',            # Default quote character\n",
        "    names=None,               # Default column names\n",
        "    skip_blank_lines=True,    # Default behavior to skip empty lines\n",
        "    skiprows=[1, 2],          # Ignore specific lines by their number\n",
        "    usecols=None,             # Default to read all columns\n",
        "    parse_dates=False,        # Default to not parse dates\n",
        "    date_format=None,         # Default to automatic parsing when parse_dates is True\n",
        "    dtype=None                # Default to infer data types\n",
        ")\n",
        "\n",
        "answ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PubK-fp8jc7R"
      },
      "source": [
        "### Why These Options May Not Always Be Ideal:\n",
        "\n",
        "1. **`usecols`**:\n",
        "   - Requires the exact spelling of column names, including spaces or unusual characters.\n",
        "   - Errors in column names can lead to missing or incorrect data, which may go unnoticed.\n",
        "\n",
        "2. **`dtype`**:\n",
        "   - Works only if the data can be easily converted to the specified types.\n",
        "   - If data doesn't fit the specified type, it may be dropped silently, leading to data loss.\n",
        "   - Casting data types after reading the file allows for easier debugging and greater flexibility, as we will demonstrate later. Pandas' default behavior of inferring types is often sufficient and robust.\n",
        "\n",
        "3. **`parse_dates`** and **`date_format`**:\n",
        "   - Only works if the dates are properly formatted and consistently structured in the source file.\n",
        "   - Requires prior experience with date parsing and a clear understanding of the file's date format.\n",
        "   - It's often more practical to manually cast date columns after loading the file, as we’ll explore in future examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmcLXOPMjc7R"
      },
      "source": [
        "# Fixing Column Names\n",
        "\n",
        "Fixing column names is important for the following reasons:\n",
        "\n",
        "- **Use dot notation for accessing columns**: Cleaning column names allows us to use the dot notation (e.g., `answ.column_name`) instead of bracket notation (`answ['column name']`). This makes the code cleaner and more readable.\n",
        "- **Keep things consistent**: By applying transformations like stripping whitespace, converting to lowercase, and replacing spaces with underscores, column names become standardized. This consistency helps avoid confusion and errors when working with datasets from different sources.\n",
        "- **Avoid worrying about spaces or capitalization**: Spaces, special characters, and inconsistent capitalization in column names can lead to hard-to-debug errors. By normalizing column names, we remove these potential pitfalls and simplify data manipulation.\n",
        "\n",
        "### Apply your knowlege: **Harmonise Column names**:\n",
        "1. Ensure that all columns are harmonically named, concretely:\n",
        "    - Whitespaces are stripped from the beginning and end of column names.\n",
        "    - All column names are lowercase.\n",
        "    - Spaces are replaced with underscores for easier access using dot notation.\n",
        "    - Parentheses are removed to eliminate special characters that could cause issues.\n",
        "\n",
        "Note: Build your modification query of the columns first, before you overwrite the current column names!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RN9LuflVjc7R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5FTKx6froNZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JNtNNLYepg01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aORr83yUjc7R"
      },
      "source": [
        "# Dealing with messy columns\n",
        "\n",
        "Our DataFrame currently contains many columns that are not relevant to our analysis. For example, the column `os_4_text` is completely empty and provides no useful information. Since it serves no purpose, we can safely remove it to simplify our dataset.\n",
        "\n",
        "## Apply your knowlege - **Removing columns**\n",
        "2. Remove the column `os_4_text` from the answ dataframe\n",
        "    - Do not remove it immediately. Try the command first. Once happy with the result assign it to answ\n",
        "    - Do not use `inplace`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqssRSe6jc7R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jrRryrtWqQ14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWntEDP4jc7R"
      },
      "source": [
        "## Why You Should Avoid Using `inplace`\n",
        "\n",
        "As mentioned earlier, it's good practice to first check the result of any operation before updating your DataFrame. While many Pandas functions offer the `inplace` argument to modify the DataFrame directly. In this case it could have saved us having to save the result of the drop operation to our answ dataframe.\n",
        "\n",
        "`answ.drop(columns=columns_to_drop, inplace=True)`\n",
        "has the same result as\n",
        "`answ = answ.drop(columns=columns_to_drop)`\n",
        "\n",
        "I recommend _not to use inplace_. Here's why:\n",
        "\n",
        "1. Write your operation in a separate cell and execute it to preview the result.\n",
        "2. Check the output:\n",
        "   - If **No**, adjust your code and test again.\n",
        "   - If **Yes**, overwrite the DataFrame with the result explicitly.\n",
        "\n",
        "This approach, as discussed in the previous section, minimizes errors and ensures you maintain control over your dataset. Avoiding `inplace` makes your workflow safer and easier to debug."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply your knowlege - Renaming individual columns\n",
        "3. Rename `recipientlastname` to `lastname` and `finished` to `completed`"
      ],
      "metadata": {
        "id": "wGoNJndYqaYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HZv8yvuTtL7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NIpeiuJjc7R"
      },
      "source": [
        "## Identifying Columns to Delete\n",
        "\n",
        "Typically, datasets contain many columns. When working with \"real\" data instead of cleaned datasets, you will often find that only a fraction of the columns are relevant for your analysis. Viewing your DataFrame with unnecessary columns can overwhelm you with information that isn't useful.  \n",
        "\n",
        "To be mindful of your cognitive resources, it is good practice to delete what you do not need, allowing you to focus on what is essential. So, how can you determine which columns are relevant?\n",
        "\n",
        "1. Find columns that contain no information or always the same information.\n",
        "2. Identify columns that contain irrelevant information.\n",
        "\n",
        "### Finding Columns That Contain No Information\n",
        "\n",
        "The function `nunique()`, short for \"number of unique values,\" shows how many unique values each column has. By running this command, we can identify columns that:\n",
        "\n",
        "- Are empty (0 unique values), e.g., `recipientfirstname`.\n",
        "- Always contain the same value (1 unique value), e.g., `userlanguage`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDVhvhsAjc7R"
      },
      "outputs": [],
      "source": [
        "answ.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHUhMcPzjc7S"
      },
      "source": [
        "## Apply your knowlege - **Deleting Columns That Contain No Information**\n",
        "\n",
        "4. Delete all columns which:\n",
        "    - contain no information.\n",
        "    - always contain the same information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozjg09nIjc7S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9YRM-c8jc7S"
      },
      "source": [
        "# Fixing Column Data Types\n",
        "## Apply your knowlege - Identifying wrong data types\n",
        "5. Examine our resulting DataFrame. For each column, consider whether the data type matches the appropriate scale type. Remember the following relationships between data types and scale types:\n",
        "    - **bool**: Nominal\n",
        "    - **object**: Nominal\n",
        "    - **int/float**: Interval or metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jTYHkuDjc7S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uR7lak0jc7S"
      },
      "source": [
        "# Casting to nominal/string: 4 Options\n",
        "\n",
        "There are three ways to fix this:\n",
        "\n",
        "1. Simple Conversion to string (Quick and easy, but has problems)\n",
        "2. Simple Conversion to a categorial variable (Quick and easy, but labels might be uninformative)\n",
        "3. Find numerical values and replace with string labels\n",
        "4. Combine 3&4: Create labels, then convert to categorial (Recommended!)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHPB89yQjc7V"
      },
      "source": [
        "# Option 1 - Simple String Conversion\n",
        "\n",
        "Simple String Conversion is very straightforward. We only have to call the function `astype(\"str\")` on the column we want to convert.\n",
        "\n",
        "For example, in our survey dataset, the `os` column represents the operating system each participant worked with the most. Here is the mapping:\n",
        "\n",
        "\n",
        "* `1.0` is `Windows`\n",
        "* `2.0` is `MacOS`\n",
        "* `3.0` is `Linux`\n",
        "* `4.0` is `Other`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymv7fc_7jc7V"
      },
      "source": [
        "## Apply your knowlege - Simple String conversion\n",
        "6. Convert the column `os` to a string using `astype(\"str\")`.\n",
        "16. Use the converted column to create a pie chart. Is the result as expected? - do you see an issue with simple string conversion?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDilr6zKjc7V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v2kKFL8tnI3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Option 2 - Casting to Categorical\n",
        "String is not the only datatype in Pandas suitbale for nominal data. In fact, the Pandas introduced its own datatype `pd.Categorial` to adress the issue with nominal (aka categorical) data.\n",
        "\n",
        "## Apply your knowlege:\n",
        "17. Use the function `pd.Categorial` to convert `os` to be categorial\n",
        "18. Create a pie chart with the categorial data"
      ],
      "metadata": {
        "id": "8AzmlF23ncyV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OY5C4h2zm8s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nYQlYBmYot01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PXBZbG8jc7V"
      },
      "source": [
        "# Option 3 - Replace with labelled string\n",
        "A better option to simple conversion to string or categorical is to repace the number with a meaning full label, that is a string that tells us what this value represents. Recall the following mapping:\n",
        "* `1.0` is `Windows`\n",
        "* `2.0` is `MacOS`\n",
        "* `3.0` is `Linux`\n",
        "* `4.0` is `Other`\n",
        "\n",
        "## Apply your knowlege - Replace values\n",
        "8. Replace the numbers of `os` with strings that represent the operating system.\n",
        "9. Create a pie chart using the fixed column. What improvements do you see?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wujYRU6fjc7V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ljooyp7VqnN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Option 4: Categorical with Labels\n",
        "Since the column is still a string we can also convert it now to a `pd.Categoriacal`.\n",
        "## Apply your knowlege\n",
        "10. Now use the column with the replaced values to create a categorical column.\n",
        "11. Create a pie chart once again.\n",
        "\n",
        "Note: Since a categorical column does not allow any values that were not defined beforehand, undefined values will automatically be treated as missing."
      ],
      "metadata": {
        "id": "brJIfls4loAc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RRCAr8WEeH2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x13lcHAyjc7W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply your knowlege - **Fix sex column**\n",
        "The column `sex` is also encoded incorrectly (`0` is `Male` and `1` is `Female`)\n",
        "\n",
        "12. Convert the numerical column `gender` from int to string or categorical.\n",
        "13. Create a Pie Chart with the correct labels\n"
      ],
      "metadata": {
        "id": "80vzF-_4xVzv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lY13hROjc7T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xWnBXRfwzHiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksOvcP4ejc7T"
      },
      "source": [
        "# Creating Ordinal Columns\n",
        "\n",
        "Ordinal data is always tricky. Unlike nominal data, there is no clear best practice for handling it.  \n",
        "\n",
        "For example, consider the education column, which is currently encoded as an integer. This presents the same issue as the gender column: the labels are numeric, and every time we look at the data, we must remember what each number represents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZrno7KGjc7T"
      },
      "outputs": [],
      "source": [
        "answ.education"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19161thFjc7T"
      },
      "source": [
        "To fix this, we can recode the values again, replacing numbers with strings. We can also use `Categorical` again, but this time we set the optional argument `ordered=True`.\n",
        "\n",
        "## Important: Wrong Ordering\n",
        "\n",
        "Pay close attention to the resulting ordered category that we get:\n",
        "\n",
        "- `Bachelor < High School < Master`\n",
        "\n",
        "This ordering is incorrect. The next cell will demonstrate how to fix this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7nS1Hbwjc7T"
      },
      "outputs": [],
      "source": [
        "recode = {1: 'High School',\n",
        "          2: 'Bachelor',\n",
        "          3: 'Master',\n",
        "          4: 'PhD'}\n",
        "\n",
        "pd.Categorical(\n",
        "    answ.education.replace(recode),\n",
        "    ordered=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S2Z20Oljc7T"
      },
      "source": [
        "## Defining the Order of Categories\n",
        "\n",
        "The function `Categorical` will attempt to determine the order of categories on its own if you do not specify it. By default, it will sort all unique values. Since the values are strings after recoding, they will be ordered alphabetically.\n",
        "\n",
        "It took me a while to find a best practice for recoding values and converting them into an ordered categorical with the correct order. Here is my approach:\n",
        "\n",
        "1. Create a `recode` dictionary. This dictionary needs to have the following properties:\n",
        "    - Keys and values must be listed in ascending order.\n",
        "    - Every possible key-value pair must be included, even if it does not appear in the data (e.g., `PhD` in this case).\n",
        "    - Keys are the values you want to replace (e.g., numbers).\n",
        "    - Values are strings that provide the labels for the categories.\n",
        "2. Use `replace` with the `recode` dictionary to update the column.\n",
        "3. Provide the optional argument `categories` and set it to the dictionary’s values. The order of values will match the order you define in the dictionary, ensuring the correct sequence.\n",
        "4. Set `ordered=True` to make the categories ordinal.\n",
        "\n",
        "The cell below demonstrates this approach. You can adapt it to your needs for reuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaQDId7bjc7U"
      },
      "outputs": [],
      "source": [
        "recode = {1: 'High School',\n",
        "          2: 'Bachelor',\n",
        "          3: 'Master',\n",
        "          4: 'PhD'}\n",
        "\n",
        "pd.Categorical(\n",
        "    answ.education.replace(recode),\n",
        "    categories=recode.values(),\n",
        "    ordered=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvA_ITtyjc7U"
      },
      "source": [
        "## Limited Numerical Functions with Ordered Categoricals\n",
        "\n",
        "Once you have created an ordered categorical, the only mathematical functions that remain available are `min` and `max`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQD4hQPEjc7U"
      },
      "outputs": [],
      "source": [
        "print(f\"Education Level - Min: {answ.education.min()} Max: {answ.education.max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3sLRsNKjc7U"
      },
      "source": [
        "But this can be problematic if we need more operations. Take the columns `python_knowlege` and `r_knowlege`, for example. We can easily calculate statistics for these columns when they are numerical. By examining the quantiles, we can observe that the knowledge levels were distributed very differently. However, we still have to remember what the numbers represent.\n",
        "\n",
        "**Note:** If you cannot interpret this output, revisit the section on descriptive statistics. You should be able to read and interpret this at this point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlFxPbWwjc7U"
      },
      "outputs": [],
      "source": [
        "answ[[\"python_knowlege\", \"r_knowlege\"]].quantile([0, 0.25, 0.5, 0.75, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhfC9tgxjc7U"
      },
      "source": [
        "Now, one might think it is a good idea to recode these variables as well. I demonstrate this approach below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4UT04t4jc7U"
      },
      "outputs": [],
      "source": [
        "recode = {0: 'No knowledge',\n",
        "          1: 'Slightly knowledgeable',\n",
        "          2: 'Moderately knowledgeable',\n",
        "          3: 'Very knowledgeable',\n",
        "          4: 'Extremely knowledgeable'}\n",
        "\n",
        "python_recoded = pd.Categorical(\n",
        "    answ.python_knowlege.replace(recode),\n",
        "    categories=recode.values(),\n",
        "    ordered=True)\n",
        "\n",
        "r_recoded = pd.Categorical(\n",
        "    answ.r_knowlege.replace(recode),\n",
        "    categories=recode.values(),\n",
        "    ordered=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0u2FAbbjc7U"
      },
      "source": [
        "However, after recoding, we can no longer calculate the median because pandas cannot compute the median from strings.  \n",
        "(Honestly, this is somewhat inconvenient, but that’s the way it works.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zydCfG7Fjc7U"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    python_recoded.median()\n",
        "except Exception as e:\n",
        "    print(f\"Caught error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pds0QGWjc7U"
      },
      "source": [
        "The only operation still available is `value_counts()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqvoHiasjc7U"
      },
      "outputs": [],
      "source": [
        "python_recoded.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_CB8Y_Rjc7U"
      },
      "source": [
        "## Maintaining Numerical Functions\n",
        "\n",
        "Even though it is not ideal, I recommend avoiding recoding if you want to maintain numerical functionality.  \n",
        "\n",
        "In this example, I use numbers as category labels when converting to an ordered `Categorical`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-B1CDd1jc7U"
      },
      "outputs": [],
      "source": [
        "answ.r_knowlege = pd.Categorical(answ.r_knowlege,\n",
        "                                 categories=[0, 1, 2, 3, 4],\n",
        "                                 ordered=True)\n",
        "answ.python_knowlege = pd.Categorical(answ.python_knowlege,\n",
        "                                 categories=[0, 1, 2, 3, 4],\n",
        "                                 ordered=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Xcmwhcjc7U"
      },
      "source": [
        "This approach allows me to easily convert the data back to numbers, enabling calculations such as the median.  \n",
        "\n",
        "Note the use of the `astype(int)` command in this process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwpuQWjCjc7U"
      },
      "outputs": [],
      "source": [
        "answ.python_knowlege.astype(\"int\").median()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFfPcPxTjc7U"
      },
      "source": [
        "Of course, this does not solve the issue of having to remember what each value represents.  \n",
        "\n",
        "We can address this by recoding the output with our labels. Here, we follow the same steps as at the beginning of this section. However, since the knowledge columns are categorical, we use `astype(int)` to enable the use of the `quantiles` function.  \n",
        "\n",
        "Finally, we apply `replace` to the output to display labels instead of numbers in the quantiles table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehS6AlsEjc7U"
      },
      "outputs": [],
      "source": [
        "recode = {0.0: 'No knowledge',\n",
        "          1.0: 'Slightly knowledgeable',\n",
        "          2.0: 'Moderately knowledgeable',\n",
        "          3.0: 'Very knowledgeable',\n",
        "          4.0: 'Extremely knowledgeable'}\n",
        "\n",
        "answ[[\"python_knowlege\", \"r_knowlege\"]].astype(\"int\").quantile([0, 0.25, 0.5, 0.75, 1]).replace(recode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2ozKmhujc7U"
      },
      "source": [
        "# Converting to Datetime\n",
        "\n",
        "One thing that `read_csv` does not do automatically for us is convert dates.  \n",
        "\n",
        "That being said, there is an option to provide the optional argument `parse_dates`, where we can list the columns that should be converted. We can also use `date_format` to specify how we want the dates to be formatted.  \n",
        "\n",
        "I leave it to you to try this out. For beginners, I recommend converting dates after reading in the CSV file. This approach makes it easier to apply trial and error and reduces the likelihood of encountering errors you do not understand.  \n",
        "\n",
        "Converting non-exotic dates is straightforward this way.\n",
        "\n",
        "## Apply your knowlege - Convertig to Datetime with format detection\n",
        "14. Use the function `to_datetime` to convert the `startdate` column from string to datetime. Let Pandas try to guess the date format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoeJQmOqjc7U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VrVlqu-jc7U"
      },
      "source": [
        "You can explicitly define the date format by providing the optional argument `format` in the `to_datetime` function.  \n",
        "\n",
        "For figuring out the correct format, I recommend copying an example date and asking ChatGPT which format you should use so pandas can convert it correctly. Below, we define a format that pandas would have been able to infer automatically:\n",
        "\n",
        "`%Y-%m-%d %H:%M:%S`:\n",
        "- `%Y`: Four-digit year (e.g., 2024)\n",
        "- `%m`: Two-digit month (e.g., 10)\n",
        "- `%d`: Two-digit day (e.g., 15)\n",
        "- `%H`: Two-digit hour in 24-hour format (e.g., 16)\n",
        "- `%M`: Two-digit minute (e.g., 59)\n",
        "- `%S`: Two-digit second (e.g., 04)\n",
        "\n",
        "## Apply your knowlege - Convertig to Datetime with explicit format defenition\n",
        "\n",
        "15. Use the function `to_datetime` to convert the `enddate` column from string to datetime. This time figure out the appropirate date format yourself and define it explicitly by setting the optional argument `format`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhW-itZ3jc7V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MOPhCNcjc7V"
      },
      "source": [
        "But why would we even care to convert dates and times?  \n",
        "\n",
        "Converting dates and times gives us access to time-based calculations.\n",
        "\n",
        "## Apply your knowlege - Time-based Calculations.\n",
        "\n",
        "16. Subtract timestamps the two timestamps from one another so you obtain a measure of how long each respondent took to complete the questionnaire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIK4zLZwjc7V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij8sAAjXjc7V"
      },
      "source": [
        "## Creating New Columns\n",
        "\n",
        "The previous example was a perfect scenario for when we may want to create a new column. This process is very straightforward:  \n",
        "\n",
        "All we need is a pandas `Series` that has the same length (number of rows) as our DataFrame. We can then create a new column as follows:\n",
        "\n",
        "```python\n",
        "df[\"new_column_name\"] = SERIES"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply your knowlege - Time-based Calculations.\n",
        "\n",
        "17. Create a new column that contains how long each respondent took to complete the questionnaire."
      ],
      "metadata": {
        "id": "oFTfrhbFPOI-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17-46tgcjc7V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yWRgiWHjc7V"
      },
      "source": [
        "# Casting to Numeric\n",
        "\n",
        "Pandas generally does a good job detecting numerical columns in your dataset. However, if it fails, this may indicate the presence of unusual values or numbers stored as strings (e.g., within quotation marks).  \n",
        "\n",
        "Before converting, it’s a good idea to identify and handle these problematic values first. Attempting to convert a column (e.g., `revenue`) to numeric will likely raise an error if pandas couldn’t already infer the type. This suggests that the conversion isn’t straightforward and requires cleaning.\n",
        "\n",
        "Below, I create a simple example (a pandas Series) by hand to demonstrate this concept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR0Fp8uHjc7V"
      },
      "outputs": [],
      "source": [
        "# Example data\n",
        "house_numbers = pd.Series(['10', '20.5', 'thirty', '40', 'NaN', '', '50a'], name='revenue')\n",
        "house_numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7pNFNpfjc7V"
      },
      "source": [
        "## Apply your knowlege - Converting to numerical\n",
        "18. Use the function `to_numeric()` to convert `house_numbers` to numerical. Inspect the error message. Describe in your own words what the issue was.\n",
        "However, it raises an error because it cannot parse `\"thirty\"`.\n",
        "\n",
        "\n",
        "Note: This behavior is typical when working with CSV files: any column that can be converted to numeric without errors will already have been converted automatically during the file read process.  \n",
        "\n",
        "The error simply confirms that there is an issue with the data that needs to be addressed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k3aRZ2kjc7V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMwlx0cNjc7V"
      },
      "source": [
        "What many people do is simply ignore these errors and convert the data anyway.  \n",
        "\n",
        "## Apply your knowlege\n",
        "19. Use the optional argument `errors='coerce'` of the `to_numeric()` function to force pandas to convert all values it can convert. Inspect the result - what happens to the values that could not be converted. Can you think of scenarios were this might be problematic?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8dYYZWrjc7V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KJipRPUjc7V"
      },
      "source": [
        "If you want to be diligent, you can compare the values before and after the conversion.  \n",
        "\n",
        "In particular, you can filter for rows that were not empty before the conversion but are empty after the conversion. This helps identify problematic values that were lost during the conversion process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3BMoyKBjc7V"
      },
      "outputs": [],
      "source": [
        "house_numbers[house_numbers.notna() & converted_values.isna()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqptpWvEjc7V"
      },
      "source": [
        "The list above shows us which values are problematic:\n",
        "\n",
        "- **`thirty`**: Should be replaced with `30` for the conversion to work.\n",
        "- **`NaN`**: Can be converted to a missing value.\n",
        "- **`Empty`**: Can be converted to a missing value.\n",
        "- **`50a`**: Should be replaced with `50` for the conversion to work.\n",
        "\n",
        "This means there are only two replacements we need to make to avoid any information loss during the conversion.\n",
        "\n",
        "## Apply your knowlege - Manually fixing values\n",
        "20. Replace the values `thirty` and `50a` by numbers manually.\n",
        "21. Convert the column to numerical and inpect the result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DbpHt0fjc7V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZL-gR09jc7V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t20piFZjc7V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change of Dataset: Creditcards\n",
        "For the next section we are going to use a different dataset, a list of imaginary clients and their credit card numbers."
      ],
      "metadata": {
        "id": "job8EELy8XBG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1p0c03IMjc7W"
      },
      "outputs": [],
      "source": [
        "cards_url = \"https://raw.githubusercontent.com/mschuessler/dba_lecture/refs/heads/main/data/customer_credit_cards.csv\"\n",
        "cards = pd.read_csv(cards_url)\n",
        "cards"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply your knowlege - Find incorrect datatypes\n",
        "22. Inspect the datatypes of the columns of cards. Which ones are in an incorrect format?"
      ],
      "metadata": {
        "id": "vQnqWHTy6tTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "96ddnood6soR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUQWMZfAjc7W"
      },
      "source": [
        "# Casting to Bool\n",
        "\n",
        "If a value encodes \"yes\" or \"no,\" it may be useful to cast it to a boolean type for easier counting and filtering.  \n",
        "\n",
        "When \"yes\" is coded as `1` and \"no\" as `0`, casting to `bool` is very straightforward. This is the case for the `active` column in our dataset.\n",
        "\n",
        "# Apply your knowlege - Direct cast to bool\n",
        "23. Cast the column `active` to be boolean\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9mgrdtLjc7W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Casting string to bool\n"
      ],
      "metadata": {
        "id": "aE-R4l4ZgGgV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUUhzaC1jc7W"
      },
      "source": [
        "For the `premium` column, the situation is slightly different: here, `\"yes\"` represents `True` and `\"no\"` represents `False`.  \n",
        "\n",
        "This can also be handled with ease. We simply check if `premium` is `\"yes\"`. The result of this comparison will be `True` for all cells that contain `\"yes\"` and `False` for all others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_inyvU-gjc7W"
      },
      "outputs": [],
      "source": [
        "cards.premium == \"yes\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JECq0Ifxjc7W"
      },
      "source": [
        "## When Numbers Aren't Really Numbers\n",
        "\n",
        "- **Default Behavior**: Columns filled exclusively with numbers are typically read as `int` or `float` by default in Pandas.\n",
        "\n",
        "- **Why This Might Not Be Ideal**: Sometimes, numerical values don't represent quantities on an interval scale and shouldn't be treated as such. This commonly occurs with numeric identifiers, such as IDs or account numbers.\n",
        "\n",
        "- **Examples of Numeric Identifiers**:\n",
        "  - ISBN numbers\n",
        "  - Credit card numbers\n",
        "  - Other encoded identifiers\n",
        "\n",
        "- **Quick Test**: Ask yourself: *If I multiply this number by 2 and then subtract 1, does it still have a valid meaning?* For identifiers, the answer is usually \"no.\" In such cases, these values should be treated as nominal (strings or categorical), not numerical.\n",
        "\n",
        "### Example: Credit Card Numbers\n",
        "Consider a dataset with credit card information encoded as numerical. Multiplying a credit card number by 2 would result in an invalid number, confirming it’s an identifier, not a real number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTAWh1zRjc7W"
      },
      "source": [
        "## Risks when casting to int\n",
        "To fix columns that are ints but should be string, you can cast the column to a string, ensuring it is treated appropriately as an identifier rather than a numerical value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yCvwQD7jc7W"
      },
      "outputs": [],
      "source": [
        "cards.credit_card_number.astype(\"str\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPkTpow1jc7W"
      },
      "source": [
        "But wait—_something is not right here_.  \n",
        "\n",
        "Why do these credit card numbers have different lengths? This issue was present even before we converted them to strings, so there must be something wrong with the file, right?  \n",
        "\n",
        "Actually, yes, there is. Take a look at [`customer_credit_cards.csv`](https://github.com/mschuessler/dba_lecture/blob/main/data/customer_credit_cards.csv).  \n",
        "\n",
        "The issue is that some card numbers have leading zeros. These zeros were lost when pandas read them in as numbers because leading zeros have no numerical meaning. The problem with the file is that the credit card numbers should have been escaped with quotation marks (`\"`) to let pandas know they are strings.  \n",
        "\n",
        "Since this did not happen, we can fix it by telling pandas to read this column as a string in the first place. To do this, we use the optional argument `dtype` with a dictionary specifying the column name as the key and the data type as the value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh7_Uu9Ajc7W"
      },
      "outputs": [],
      "source": [
        "cards = pd.read_csv(cards_url,dtype={\"credit_card_number\": \"string\"})\n",
        "cards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCN5ZUUAjc7W"
      },
      "outputs": [],
      "source": [
        "cards.credit_card_number.str.len()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E97bWSg9jc7X"
      },
      "source": [
        "## Loading Titanic Dataset One More Time\n",
        "\n",
        "Let’s bring back our passengers from the Titanic. This time, we will only read in the columns needed for the following demonstrations.  \n",
        "\n",
        "You will also notice that I am taking a shortcut by fixing the data types that are incorrect immediately upon reading the CSV.  \n",
        "\n",
        "**Note:** Please only use this approach once you have mastered the more manual methods of handling data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YviXQ_qjc7X"
      },
      "outputs": [],
      "source": [
        "passengers = pd.read_csv(\"https://raw.githubusercontent.com/mschuessler/dba_lecture/refs/heads/main/data/titanic.csv\",\n",
        "                         usecols=[\"Name\",\"Age\",\"Cabin\",\"Sex\",\"Survived\"],\n",
        "                         dtype={\"Survived\": \"bool\", \"Sex\": \"category\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filters and Subsets\n",
        "So far, we have only been looking at one column at a time. This is useful as a first step to understand the data and should never be skipped. Once we feel confident we understand the columns we are interested in, we can move on to explore relationships using descriptive statistics. In many cases, we do this by splitting the data into subsets and then analyzing these subsets individually. This is one of the core principles of statistical analysis. There are two primary ways to compare subsets of the data:\n",
        "1) Creating subsets with filters\n",
        "2) Grouping datasets by a variable or criteria (this will be covered in a later section)\n",
        "\n",
        "\n",
        "## Defining Filters\n",
        "We can define filters by specifying a condition that selects certain rows from the dataset based on the values in one or more columns. Here we will focus on the \"EQUAL\" (`==`) criteria. However, there are many other criteria you may want to explore:\n",
        "\n",
        "- **Equality** (`==`)  \n",
        "- **Not equal to** (`!=`)\n",
        "- **Less than** (`<`)  \n",
        "- **Less than or equal to** (`<=`)  \n",
        "- **Greater than** (`>`)  \n",
        "- **Greater than or equal to** (`>=`)  \n",
        "- **Values that are not empty or invalid** (`notna()`)  \n",
        "- **Values that are empty or invalid** (`isna()`)  \n",
        "- **Checking for membership** (`.isin([value1, value2, ...])`)  \n",
        "- **String containment** (`str.contains('substring')`)  \n",
        "- **Logical combinations** (`&` for AND, `|` for OR, `~` for NOT)\n",
        "\n",
        "Here we simply define a filter that checks if the sex of the passenger is equal to \"male\". As a result we get a series that has the same lenght as the dataframe. Each cell show if the criteria has been met at that row. So\n",
        "- Passenger 1 is male\n",
        "- Passenger 2 is not male\n",
        "- Passenger 3 is not male\n",
        "- Passenger 4 is male\n",
        "- ...."
      ],
      "metadata": {
        "id": "asenH0znENKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passengers.Sex == \"male\""
      ],
      "metadata": {
        "id": "tezEAVL8E8BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can safe this series of bools into a variable (here called filter) to use it to plug it into our dataframe. Once the dataframe recieves a series of True and False it know its a filter an will return only the rows for which the filter criteria is true. As a result we are shown the rows that contain only male passengers."
      ],
      "metadata": {
        "id": "aO-KtERJFHOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filter = passengers.Sex == \"male\"\n",
        "passengers[filter]"
      ],
      "metadata": {
        "id": "IMoqbazRFPIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that you can also write the filter criteria into the the squared brackets without the need to create a filter variable. While this may be faster to type, it is harder to read. So I do not recommend it for beginners."
      ],
      "metadata": {
        "id": "pyivIKdNFUXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passengers[passengers.Sex == \"male\"]"
      ],
      "metadata": {
        "id": "eMKijnDSFXIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall the expression above is not that easy to read. Especially since we have to pay attention to not use double brackets and we have type passengers twice. There is a function that adresses this, were we don't need to state the obvious: We are filtering on passengers. The function is called query(). The code below produces exactly the same output as the two cells above. It is up to you how you want to filter your dataframes."
      ],
      "metadata": {
        "id": "Cu96paGlFcqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passengers.query('Sex == \"male\"')"
      ],
      "metadata": {
        "id": "1kwbmEnMFiGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case your are sruggeling with the `==` notation there is also a function that does the same thing. `eg` ist short for equal. There are eqivalent functions for other comparisions too."
      ],
      "metadata": {
        "id": "D3k7gCHqFrNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passengers.Sex.eq(\"male\")"
      ],
      "metadata": {
        "id": "VwyRCP4YFsdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we use a filter on a dataset to select only specific rows, we obtain a subset. We can save this subset as a variable and run descriptive statistic on it or create visualisations with it.\n",
        "## Apply your knowlege - Calculations on Subsets\n",
        "24. Create a subset: Filter the passengers dataset for female passengers and save this as a new dataframe\n",
        "25. Compute descriptive statisitcs for the survival rate of the female passengers\n",
        "26. Create a pie char showing the chanche of survival for female passengers"
      ],
      "metadata": {
        "id": "mXSV8XAXGVgl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5BxINdwxHqVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bGlQODFdH-Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining several filter\n",
        "We can use **Logical combinations** of filters to combine criteria. Here we create an age filter and combine it (`&`) with our filter for females"
      ],
      "metadata": {
        "id": "o4fEo6cBQEQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "above_30 = passengers.Age > 30\n",
        "passengers[above_30 & filter]"
      ],
      "metadata": {
        "id": "94azriDbqj8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz8DtXsHjc7W"
      },
      "source": [
        "# Missing Values are Anoying\n",
        "\n",
        "Missing values can be challenging because they lack inherent properties, making operations difficult. Essentially missing values have no Scale Type, so all operations we usally use dont apply to them.\n",
        "\n",
        "Note: We need to import nan here for demonstration purposes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import nan"
      ],
      "metadata": {
        "id": "N-pBtR1xKQZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing values are not on the interval or ratio scale\n",
        "If values are on the ratio scale it allows us to use **Operations**. But this does not work for missing values:  \n",
        "\n",
        "- What is `1 + NA`? Is one plus something unknown still one?  \n",
        "- What is `5 / NA`?  "
      ],
      "metadata": {
        "id": "NxKNHSoBJ05h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS6n3EWijc7X"
      },
      "outputs": [],
      "source": [
        "1 + nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvQVuqXWjc7X"
      },
      "outputs": [],
      "source": [
        "5 / nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F70n6vA4jc7X"
      },
      "source": [
        "If we don't have operations. It makes it difficult to calculate descriptive statistics.\n",
        "\n",
        "For example: What is the mean of `3, 5, 6, 1, NA, 2`?  \n",
        "\n",
        "The mean is calculated as the sum of all values divided by the number of values. However, since we cannot calculate the sum due to the presence of `NA`, the mean cannot be determined directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50kjHc6zjc7X"
      },
      "outputs": [],
      "source": [
        "(3 + 5 + 6+ 1 + nan + 2)/6"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing values are not on the ordinal scale\n",
        "Consider the following example with missing values (`NA`):  \n",
        "How should you *sort* `3, 5, 6, 1, NA, 2`?  \n",
        "\n",
        "You can't really because we do not know if NA is bigger or smaller than any other number in this list.\n",
        "\n",
        "Note: Pandas places `NA` at the end of the sorted list. However, SAS, a popular statistical software, places missing values at the beginning. This inconsistency can lead to unexpected results and differences between tools."
      ],
      "metadata": {
        "id": "mXAinAbrLAk5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm1Q0oUmjc7W"
      },
      "outputs": [],
      "source": [
        "numbers_with_na = pd.Series([3, 5, 6, 1, nan, 2])\n",
        "numbers_with_na.sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXVAKZVYjc7W"
      },
      "source": [
        "## Missing values are not on the nominal scale\n",
        "We can not even compare missing values if there are the same. Which means they are not even nominal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvemktqOjc7W"
      },
      "outputs": [],
      "source": [
        "x1 = nan\n",
        "x2 = nan\n",
        "x1 == x2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OJiVz2Pjc7W"
      },
      "source": [
        "This creates problems when we want to filter for missing values.  \n",
        "\n",
        "If we cannot compare missing values, how can we filter for them? For example, consider our previously created Series `numbers_with_na`, which contains a missing value. Yet, the following code does not work as expected:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzUIuNH-jc7W"
      },
      "outputs": [],
      "source": [
        "numbers_with_na == nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV3fxNR7jc7X"
      },
      "source": [
        "# Graceful Handeling of Missing Values\n",
        "Strictly speaking with missing values, we have a lot of problems\n",
        "*   We cant calculate parametic statistics with them: No mean, standard deviation etc.\n",
        "*   We cant sort them: No Median, Mode, Quantiles\n",
        "*   We cant compare them for equality: No filters, groups, finding them\n",
        "\n",
        "If you approach Missing values this way it is called: *strict handling of missing value*. This is the standard approach in languages such as C, Java etc.\n",
        "\n",
        "Fortunately, pandas handles missing values gracefully. This means we have specialised functions to deal with them and most common function just ignore them.\n",
        "\n",
        "So even though we could not calculate the mean by hand, if we use pandas mean function we still get a result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4CyteaQjc7X"
      },
      "outputs": [],
      "source": [
        "numbers_with_na.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bXQe41cjc7X"
      },
      "source": [
        "Most operations in pandas have a `skipna` flag, which is set to `True` by default.  \n",
        "\n",
        "Let’s see what happens when we tell pandas to be strict about missing values by setting `skipna=False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crl-KYKjjc7X"
      },
      "outputs": [],
      "source": [
        "numbers_with_na.mean(skipna=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttkYbNRLjc7W"
      },
      "source": [
        "Luckily, we also have two functions that address the issue of **filtering for missing values, or filtering them out**.\n",
        "\n",
        "- `isna()`: Identifies missing values.\n",
        "- `notna()`: Identifies non-missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLlubYs4jc7W"
      },
      "outputs": [],
      "source": [
        "numbers_with_na.isna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNJZBpazjc7W"
      },
      "outputs": [],
      "source": [
        "numbers_with_na.notna()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas even provides the function `fillna` to replace missing values"
      ],
      "metadata": {
        "id": "PzAfYpcSOPUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numbers_with_na.fillna(0)"
      ],
      "metadata": {
        "id": "xbc5MYPROZ6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3aclAsOjc7X"
      },
      "source": [
        "# Finding Columns with Missing Values\n",
        "\n",
        "Because pandas handles missing values gracefully, you may not have even noticed that we have been ignoring them all along.\n",
        "\n",
        "\n",
        "\n",
        "For example, the DataFrame info shows that there are 891 passengers, but we only know the age for 714 of them. Additionally, for a very small share of 214 passengers, we know which cabin they stayed in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DAwKYIujc7X"
      },
      "outputs": [],
      "source": [
        "passengers.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lnoE59rjc7X"
      },
      "source": [
        "This means that when we look at the age distribution, it represents only the 714 passengers whose age is known, not the full set of 891 passengers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0XM7eWOjc7X"
      },
      "outputs": [],
      "source": [
        "passengers.Age.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1Ytm-MUjc7X"
      },
      "source": [
        "## Apply your knowlege - Filtering for missing values\n",
        "27. Create a filter than finds all entries in the passenger dataset, were `Age` is missing.\n",
        "28. Create a filter than finds all entries in the passenger dataset, were `Cabin` is missing.\n",
        "29. Combine both filters to show all passengers were we have no information about their age or which cabin they traveled in\n",
        "Now, let’s look at the passengers for whom we know neither the age nor the cabin they stayed in.  \n",
        "\n",
        "This demonstrates how `isna()` and `notna()` can be very useful for filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYc-38T6jc7X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1yTtM13jc7X"
      },
      "source": [
        "We can replace missing values, but for statistical analysis and describing the data, this is usually a bad idea.  \n",
        "\n",
        "For example, we should not simply guess the ages of people. Instead, we should explicitly state how many individuals have unknown ages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7mLsJWajc7X"
      },
      "outputs": [],
      "source": [
        "print(f\"The median age of passengers was {passengers.Age.median()}. \" +\n",
        "    f\"But note that for {round(passengers.Age.isna().mean()*100,1)}%\" +\n",
        "     \" of passengers, the age is unknown.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv0wle6Bjc7X"
      },
      "source": [
        "## Apply your knowlege - Replace missing values\n",
        "\n",
        "30. For the sake of this exercise, let’s assume we know that people who did not have a cabin were very likely to stay under deck. Replace all missing values of the column `Cabin` with the value `Under deck?`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGJtIMcsjc7X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQwVDJDsjc7X"
      },
      "source": [
        "## Apply your knowlege - Creating subsets\n",
        "31. Create a subset called `only_children` that contains all passengers younger than 18\n",
        "32. Create a subset called `only_female_adults` that contains all female passengers that are at least 18 years old\n",
        "33. Create a subset called `only_male_adults` that contains all male passenger that are at least 18 years old\n",
        "34. Calculate the chanche of survival for Children, Woman and Men"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNDXAlOZjc7X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nIc8JmFjc7X"
      },
      "outputs": [],
      "source": [
        "surv_children = round(100*children.Survived.mean(),1)\n",
        "surv_woman = round(100*woman.Survived.mean(),1)\n",
        "surv_men = round(100*men.Survived.mean(),1)\n",
        "\n",
        "print(f\"Survival rates: \\n\"+\n",
        "      f\"children: {surv_children}%\\n\"+\n",
        "      f\"woman: {surv_woman}%\\n\"+\n",
        "      f\"men: {surv_men}%\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J65RQgPjc7X"
      },
      "source": [
        "To be able to differentiate between the subgroups we can add a new column that shows which subgroub our subset show. We need to call the `copy` command because right now our subset are just views on the original data. The command ensures we are actually creating a copy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9obfHnLQjc7X"
      },
      "outputs": [],
      "source": [
        "children = passengers[only_children].copy()\n",
        "woman = passengers[only_female_adults].copy()\n",
        "men = passengers[only_male_adults].copy()\n",
        "children[\"Group\"] = \"Children\"\n",
        "woman[\"Group\"] =  \"Woman\"\n",
        "men[\"Group\"] = \"Men\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSCPphOpjc7Y"
      },
      "source": [
        "## Extending DataFrame Vertically\n",
        "\n",
        "We can add more information to the end of the DataFrame using the `concat` method, short for \"concatenate.\"  \n",
        "\n",
        "To demonstrate this, we will create three subsets and combine them to one dataset again.Now, I can combine the subsets back together using `concat()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx2Ao7aBjc7Y"
      },
      "outputs": [],
      "source": [
        "grouped_passengers = pd.concat([children, woman, men])\n",
        "grouped_passengers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiWKgvwejc7Y"
      },
      "source": [
        "However, you will notice that my new DataFrame is shorter than the original one.  \n",
        "\n",
        "This is because of the filters I used. Whenever age was not defined for an entry, none of the filters selected it to be included in any of the subsets.  \n",
        "\n",
        "This highlights one of the risks of handling data in this way: you may unintentionally exclude some data without even realizing it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnl78JWJjc7Y"
      },
      "outputs": [],
      "source": [
        "print(f\"Orignal data: {len(passengers)} rows\\n\"+\n",
        "      f\"Grouped data: {len(grouped_passengers)} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEpOTm0djc7Y"
      },
      "source": [
        "# Grouping data\n",
        "\n",
        "Let’s improve our approach by working directly on the original dataset. Since we are nearing the end of the lecture, let’s combine a few things we have learned.\n",
        "\n",
        "We first create a new column that we fill with the inital value `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Pof8CXJjc7Y"
      },
      "outputs": [],
      "source": [
        "passengers[\"Group\"] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgdWabqhjc7Y"
      },
      "source": [
        "We then use `.loc()` to update this column.  \n",
        "\n",
        "The function takes two arguments:  \n",
        "1. The filter criteria.  \n",
        "2. The column to be updated for rows where the filter matches.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDHVtXSRjc7Y"
      },
      "outputs": [],
      "source": [
        "passengers.loc[only_children, \"Group\"] = \"Children\"\n",
        "passengers.loc[only_female_adults, \"Group\"] = \"Woman\"\n",
        "passengers.loc[only_male_adults, \"Group\"] = \"Men\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuX0_e72jc7Y"
      },
      "source": [
        "Optional Step: We turn our column into a categorical type.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kaBjAg_jc7Y"
      },
      "outputs": [],
      "source": [
        "passengers.Group = pd.Categorical(passengers.Group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C5VucHijc7Y"
      },
      "source": [
        "This approach keeps missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fdlir-sHjc7Y"
      },
      "outputs": [],
      "source": [
        "passengers[passengers.Group.isna()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYT2EzZCjc7Y"
      },
      "source": [
        "Calculating descriptive statistics is now very easy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WP_7TwmOjc7Y"
      },
      "outputs": [],
      "source": [
        "passengers.Survived.groupby(passengers.Group).value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8UCNSADjc7Y"
      },
      "source": [
        "## Extending DataFrame Horizontally\n",
        "\n",
        "Rather than adding more rows to a DataFrame as we did in the previous section, we sometimes want to add more columns. This is typically the case when we have two or more data sources/tables that we can connect via a key.  \n",
        "\n",
        "If you’ve used Excel before, you may be familiar with `VLOOKUP`. The pandas command [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.merge.html) works similarly. For a source DataFrame, we add all the column information from another DataFrame by connecting them using a key.  \n",
        "\n",
        "Consider the following DataFrame, which we are loading from a JSON file. It is very simple, containing an IP address and the corresponding city for each IP address."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy5sL6Zojc7Y"
      },
      "outputs": [],
      "source": [
        "ip_city = pd.read_json(\"https://raw.githubusercontent.com/mschuessler/dba_lecture/refs/heads/main/data/ip_city.json\")\n",
        "ip_city"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMzJSfk-jc7Y"
      },
      "source": [
        "## Arguments of the `merge` Command\n",
        "\n",
        "The pandas `merge` function is very powerful, and I encourage you to read the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.merge.html) and explore more usage examples online. Here, I will review some of the main arguments you can use to join data that is relevant for your analysis:\n",
        "\n",
        "- **Left DataFrame**: This is typically referred to as the primary DataFrame. It is the DataFrame to which we want to add columns. In this case, it is the answers we collected in `answ`.\n",
        "- **Right DataFrame**: This is typically referred to as the secondary DataFrame. It is the DataFrame from which we want to retrieve columns to add to the primary DataFrame.\n",
        "- **`right_on`**: The name of the column in the primary DataFrame that contains the key. For every row, pandas will look at this column and try to find a matching value in the secondary DataFrame.\n",
        "- **`left_on`**: The name of the column in the secondary DataFrame that contains the key. For every key found in the primary DataFrame, pandas will look in this column of the secondary DataFrame to find a match.\n",
        "- **`how`**: The join type. Typically, if you follow the primary-secondary logic, you want to use a Left Join:\n",
        "  - **`left`**: Keep every row of the left/primary DataFrame and add all columns of the right/secondary DataFrame. If no match is found in the secondary DataFrame, fill with `NaN`. If multiple matches are found, duplicate the row for each match and add the additional columns for each match.\n",
        "  - **`right`**: The same as `left` but in reverse. I recommend sticking with the primary-secondary logic to avoid confusion.\n",
        "  - **`inner`**: Similar to `left`, but rows with no matches in the secondary DataFrame are deleted.\n",
        "  - **`outer`**: Similar to `left`, but also adds all rows from the secondary DataFrame that did not have a match. For these, the columns from the primary DataFrame are filled with `NaN`.\n",
        "- **`suffixes`**: If both DataFrames have columns with the same name, suffixes are added to distinguish them. For example, if both DataFrames contain a column named `ip_city`, the resulting DataFrame will have two columns: `ip_city_primary` and `ip_city_secondary`.\n",
        "- **`validate`**: This optional argument helps ensure the relationship between the two DataFrames is as expected:\n",
        "  - **`one_to_one`**: Each row in the primary DataFrame should have at most one match in the secondary DataFrame.\n",
        "  - **`one_to_many`**: Each row in the primary DataFrame has a unique key, but it can match several rows in the secondary DataFrame.\n",
        "  - **`many_to_one`**: Rows in the primary DataFrame may share the same key, but for each unique key, there should be at most one match in the secondary DataFrame.\n",
        "\n",
        "### Example\n",
        "\n",
        "Here, we use the `ipaddress` column from the `answ` DataFrame to add the `ipcity` column from our secondary DataFrame, `ip_city`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGb9Yxoyjc7Y"
      },
      "outputs": [],
      "source": [
        "answers_locations = pd.merge(\n",
        "    answ, ip_city, left_on=\"ipaddress\",\n",
        "    right_on=\"ipaddress\", how=\"left\",\n",
        "    suffixes=(\"_primary\", \"_secondary\"),\n",
        "    validate=\"many_to_one\")\n",
        "answers_locations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Test\n",
        "\n",
        "## Apply your knowlege - Groupe descriptive statistics\n",
        "35. Calculate the survival rates of male and female passengers."
      ],
      "metadata": {
        "id": "3HomyFtLxRO_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "brhMe6L2xla0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K5Lnifajc7Y"
      },
      "source": [
        "## Running CHI^2 Statistical Test\n",
        "Just stating differences in descriptive statistics between groups does not imply that we have discovered to true difference. We could have just gotten lucky with our sample. In lay men terms we want to enure what we have observed is not just a random chanche outcome. Statistical these do exactly that.\n",
        "They asses if the observed outcome significantly different from the outcome that could be expected if the Null Hypothesis is true. Here our null Hyptohesis is that there is no difference in survival rates between male and females. Remember that you need to determine an approipriate statisitcal test by considering many criteria. Consult the lecture slide on this topic. ChatGpt can help you find a suitbale test if you ask the right questions. In our analysis here our outcome Varibale is ordinal (Survived/Died) and our factor is ordinal (male/femal), also it a between-groups design which is why we can use a CHI-Squared test. In the lecture we covered how to calculate this test by hand. The following code block shows how install the pingouin library which makes running statistical test really easy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pingouin"
      ],
      "metadata": {
        "id": "IZsufv-6vkcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we execute the function `chi2_independence` of the pengouin libary. We provide that function with our dataframe and tell it our factors (x) and our outcome variable (y).\n",
        "This function does something we have not seen before and which is also uncommon in most programming languages: In Python a function an return more than one results. This function returns 3 things:\n",
        "1) Expected contigency table if the Null Hypothesis would be True\n",
        "2) The observed contigency table\n",
        "3) The p-Value calulated from the CHI-Squared difference between the two tables."
      ],
      "metadata": {
        "id": "CROhw2hbyRBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pingouin\n",
        "expected, observed, p_values = pingouin.chi2_independence(passengers, x='Sex', y='Survived')"
      ],
      "metadata": {
        "id": "nSFki7xkydIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we are just interessted in the question: \"Is there a significant difference or not?\". Then we can simply check if the p-Value is smaller than 0.05. Since the chi2_independence function calculates the p-Value in several ways we need to choose one of methods. I recommend \"pearson\". The code below filters the dataframe that contains all the p-Values to consider only \"pearson\" and then check the condition if that value is smaller than 0.05. We can see that this is true. Hence, we can conclude that there was a significant difference in Chanche of Sruvial between male and female passengers."
      ],
      "metadata": {
        "id": "XgVzY8Wgyqrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_values[p_values.test==\"pearson\"].pval < 0.05"
      ],
      "metadata": {
        "id": "4J-WAfhUyr5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expected table shows how the number of male and female causalties and survivors should have been distributed, if there was no difference between genders."
      ],
      "metadata": {
        "id": "bSD9vNmGywAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expected"
      ],
      "metadata": {
        "id": "rLj8QXxSy0W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The observed table shows how they were actually distributed. Just by looking at the numbers we can see that observed differes a lot from expected."
      ],
      "metadata": {
        "id": "rv1IimQ_zEfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observed"
      ],
      "metadata": {
        "id": "wrjatPUOzFdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus: CHI^2 Test for Survival Rate of Children, Woman and Men"
      ],
      "metadata": {
        "id": "rITAmQ3jzR9c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-uJ2DBhjc7Y"
      },
      "outputs": [],
      "source": [
        "_, _, p_values = pingouin.chi2_independence(passengers, x='Group', y='Survived')\n",
        "p_values[p_values.test==\"pearson\"].pval < 0.05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_tUZoCIjc7Y"
      },
      "source": [
        "### Bonus: Focused Comparison\n",
        "\n",
        "If we have more than two groups, we need to perform a focused comparison.  \n",
        "\n",
        "So far, we know there is a significant difference between the groups, but we don’t know which groups differ from one another. We can determine this by:\n",
        "\n",
        "- Comparing men vs. women.  \n",
        "- Comparing men vs. children.  \n",
        "- Comparing children vs. women."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Byyd4VB-jc7Y"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "\n",
        "_, _, p_values = chi2_independence(passengers.query('Group != \"Woman\"'), x='Group', y='Survived')\n",
        "results[\"Men vs. Children\"] = p_values[p_values.test==\"pearson\"].pval < 0.05/3\n",
        "\n",
        "\n",
        "_, _, p_values = chi2_independence(passengers.query('Group != \"Men\"'), x='Group', y='Survived')\n",
        "results[\"Woman vs. Children\"] = p_values[p_values.test==\"pearson\"].pval < 0.05/3\n",
        "\n",
        "_, _, p_values = chi2_independence(passengers.query('Group != \"Children\"'), x='Group', y='Survived')\n",
        "results[\"Mean vs. Woman\"] = p_values[p_values.test==\"pearson\"].pval < 0.05/3\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMKTpYx8jc7Y"
      },
      "source": [
        "Hence, we can conclude that there was a significant difference between all three groups:  \n",
        "\n",
        "- The survival rate of women (77%) was significantly higher than that of children (54%) and men (18%).  \n",
        "- The difference in survival rates between men and children was also significant."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dba",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}